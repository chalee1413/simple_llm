# Open-Source Models (Default - No API keys required)
USE_OPENSOURCE_MODELS=true
HF_LLM_MODEL=gpt2

# Optional: HuggingFace Token (for private models)
# HUGGINGFACE_HUB_TOKEN=your_token_here

# Optional: OpenAI Configuration
# USE_OPENSOURCE_MODELS=false
# OPENAI_API_KEY=your_key_here
# OPENAI_MODEL=gpt-4-turbo-preview

# Optional: AWS Bedrock Configuration
# USE_OPENSOURCE_MODELS=false
# AWS_ACCESS_KEY_ID=your_key_here
# AWS_SECRET_ACCESS_KEY=your_secret_here
# AWS_REGION=us-east-1
# BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0

# Evaluation Configuration
EVAL_BATCH_SIZE=8
EVAL_MAX_SAMPLES=100
STATISTICAL_CONFIDENCE_LEVEL=0.95
BOOTSTRAP_ITERATIONS=1000

# Toxicity Detection
TOXICITY_THRESHOLD=0.7
# PERSPECTIVE_API_KEY=your_key_here

# Code Quality
MCCABE_COMPLEXITY_THRESHOLD=10
COGNITIVE_COMPLEXITY_THRESHOLD=15

# Logging
LOG_LEVEL=INFO

# RLHF Configuration
# SFT Configuration
SFT_LEARNING_RATE=2e-5
SFT_BATCH_SIZE=4
SFT_GRADIENT_ACCUMULATION_STEPS=4
SFT_NUM_EPOCHS=3
SFT_WARMUP_STEPS=100
SFT_WEIGHT_DECAY=0.01
SFT_MAX_SEQ_LENGTH=512

# Reward Model Configuration
REWARD_MODEL_LEARNING_RATE=1e-5
REWARD_MODEL_BATCH_SIZE=4
REWARD_MODEL_GRADIENT_ACCUMULATION_STEPS=4
REWARD_MODEL_NUM_EPOCHS=1
REWARD_MODEL_WARMUP_STEPS=100

# PPO Configuration
PPO_LEARNING_RATE=1e-5
PPO_BATCH_SIZE=8
PPO_MINIBATCH_SIZE=2
PPO_GRADIENT_ACCUMULATION_STEPS=4
PPO_NUM_EPOCHS=4
PPO_KL_PENALTY=0.1
PPO_CLIP_RANGE=0.2
PPO_VALUE_COEF=0.1
PPO_ENTROPY_COEF=0.01
PPO_GAMMA=1.0
PPO_LAM=0.95
PPO_MAX_SEQ_LENGTH=512

# DPO Configuration
DPO_LEARNING_RATE=1e-5
DPO_BATCH_SIZE=4
DPO_GRADIENT_ACCUMULATION_STEPS=4
DPO_NUM_EPOCHS=1
DPO_BETA=0.1
DPO_MAX_SEQ_LENGTH=512

# RLHF Training Configuration
RLHF_ALGORITHM=ppo
RLHF_CHECKPOINT_STEPS=100
RLHF_EVAL_STEPS=50
RLHF_SAVE_STEPS=100
RLHF_LOGGING_STEPS=10
RLHF_PREFERENCE_BATCH_SIZE=10
RLHF_MIN_PREFERENCES=100

